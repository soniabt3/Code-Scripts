<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>f78442696b904ab0b906ece239503963</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<div class="cell markdown" id="QS1y1fhjl86Q">
<p><strong>Team Members:</strong></p>
<p>Addepalli Sai Srinivas - 2023DA04564</p>
<p>Saurabh Singh - 2023DA04002</p>
<p>Manav V Shenoy - 2023da04246</p>
<p>Sachindra Kumar Singh - 2022dc04511</p>
<p>Sonia Benny Thomas - 2023DA04169</p>
<p><strong>Assignment:</strong></p>
<p>Assignment Set 3 (Customer Financial Data Analysis)</p>
</div>
<section id="assignment-part-1" class="cell markdown" id="UnKYnMVjfp3p">
<h1>Assignment Part 1</h1>
</section>
<div class="cell markdown" id="tvzM0ZkbnGSC">
<p>#1. Import Libraries/Dataset</p>
</div>
<div class="cell markdown" id="DrOtGnARNR7r">
<p>Importing Libraries</p>
</div>
<div class="cell code" id="OXtU233Flf48">
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Loading all the necessary libraries</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> skew</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> MinMaxScaler, StandardScaler</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split, cross_val_score, GridSearchCV</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeClassifier</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> make_scorer, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_selection <span class="im">import</span> f_classif, SelectKBest</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.neighbors <span class="im">import</span> KNeighborsClassifier</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.naive_bayes <span class="im">import</span> GaussianNB</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestClassifier, AdaBoostClassifier</span></code></pre></div>
</div>
<div class="cell markdown" id="9xpzmxz_NTod">
<p>Download the dataset</p>
</div>
<div class="cell code" id="8m9YAMxhnRBQ">
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Reading the dataset</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">&quot;Customer_Financial_Info.csv&quot;</span>)</span></code></pre></div>
</div>
<section id="2-data-visualization-and-exploration" class="cell markdown"
id="oxwxUP0gnm_b">
<h1>2. Data Visualization and Exploration</h1>
</section>
<div class="cell markdown" id="sV58I8ZyNWNX">
<p>First 2 rows for sanity check to identify all the features present in
the dataset and if the target matches with them</p>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:132}"
id="GhZWcdISnkAu" data-outputId="54ced0e4-34c3-42e2-c824-6026982a39a7">
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print first 2 rows of data</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>df.head(<span class="dv">2</span>)</span></code></pre></div>
<div class="output execute_result" data-execution_count="43">

  <div id="df-786d7e86-d116-4dbc-888f-c478da32043a" class="colab-df-container">
    <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Customer_ID</th>
      <th>Age</th>
      <th>Years_Experience</th>
      <th>Annual_Income</th>
      <th>ZIP_Code</th>
      <th>Family_size</th>
      <th>Avg_Spending</th>
      <th>Education_Level</th>
      <th>Mortgage</th>
      <th>Has_Consumer_Loan</th>
      <th>Has_Securities_Account</th>
      <th>Has_CD_Account</th>
      <th>Uses_Online_Banking</th>
      <th>Has_CreditCard</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>25</td>
      <td>1</td>
      <td>49</td>
      <td>91107</td>
      <td>4</td>
      <td>1.6</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>45</td>
      <td>19</td>
      <td>34</td>
      <td>90089</td>
      <td>3</td>
      <td>1.5</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
    <div class="colab-df-buttons">

  <div class="colab-df-container">
    <button class="colab-df-convert" onclick="convertToInteractive('df-786d7e86-d116-4dbc-888f-c478da32043a')"
            title="Convert this dataframe to an interactive table."
            style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px" viewBox="0 -960 960 960">
    <path d="M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z"/>
  </svg>
    </button>

  <style>
    .colab-df-container {
      display:flex;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    .colab-df-buttons div {
      margin-bottom: 4px;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

    <script>
      const buttonEl =
        document.querySelector('#df-786d7e86-d116-4dbc-888f-c478da32043a button.colab-df-convert');
      buttonEl.style.display =
        google.colab.kernel.accessAllowed ? 'block' : 'none';

      async function convertToInteractive(key) {
        const element = document.querySelector('#df-786d7e86-d116-4dbc-888f-c478da32043a');
        const dataTable =
          await google.colab.kernel.invokeFunction('convertToInteractive',
                                                    [key], {});
        if (!dataTable) return;

        const docLinkHtml = 'Like what you see? Visit the ' +
          '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
          + ' to learn more about interactive tables.';
        element.innerHTML = '';
        dataTable['output_type'] = 'display_data';
        await google.colab.output.renderOutput(dataTable, element);
        const docLink = document.createElement('div');
        docLink.innerHTML = docLinkHtml;
        element.appendChild(docLink);
      }
    </script>
  </div>


<div id="df-4548a829-56fe-48db-a68d-239dc03eb36a">
  <button class="colab-df-quickchart" onclick="quickchart('df-4548a829-56fe-48db-a68d-239dc03eb36a')"
            title="Suggest charts"
            style="display:none;">

<svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
     width="24px">
    <g>
        <path d="M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z"/>
    </g>
</svg>
  </button>

<style>
  .colab-df-quickchart {
      --bg-color: #E8F0FE;
      --fill-color: #1967D2;
      --hover-bg-color: #E2EBFA;
      --hover-fill-color: #174EA6;
      --disabled-fill-color: #AAA;
      --disabled-bg-color: #DDD;
  }

  [theme=dark] .colab-df-quickchart {
      --bg-color: #3B4455;
      --fill-color: #D2E3FC;
      --hover-bg-color: #434B5C;
      --hover-fill-color: #FFFFFF;
      --disabled-bg-color: #3B4455;
      --disabled-fill-color: #666;
  }

  .colab-df-quickchart {
    background-color: var(--bg-color);
    border: none;
    border-radius: 50%;
    cursor: pointer;
    display: none;
    fill: var(--fill-color);
    height: 32px;
    padding: 0;
    width: 32px;
  }

  .colab-df-quickchart:hover {
    background-color: var(--hover-bg-color);
    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);
    fill: var(--button-hover-fill-color);
  }

  .colab-df-quickchart-complete:disabled,
  .colab-df-quickchart-complete:disabled:hover {
    background-color: var(--disabled-bg-color);
    fill: var(--disabled-fill-color);
    box-shadow: none;
  }

  .colab-df-spinner {
    border: 2px solid var(--fill-color);
    border-color: transparent;
    border-bottom-color: var(--fill-color);
    animation:
      spin 1s steps(1) infinite;
  }

  @keyframes spin {
    0% {
      border-color: transparent;
      border-bottom-color: var(--fill-color);
      border-left-color: var(--fill-color);
    }
    20% {
      border-color: transparent;
      border-left-color: var(--fill-color);
      border-top-color: var(--fill-color);
    }
    30% {
      border-color: transparent;
      border-left-color: var(--fill-color);
      border-top-color: var(--fill-color);
      border-right-color: var(--fill-color);
    }
    40% {
      border-color: transparent;
      border-right-color: var(--fill-color);
      border-top-color: var(--fill-color);
    }
    60% {
      border-color: transparent;
      border-right-color: var(--fill-color);
    }
    80% {
      border-color: transparent;
      border-right-color: var(--fill-color);
      border-bottom-color: var(--fill-color);
    }
    90% {
      border-color: transparent;
      border-bottom-color: var(--fill-color);
    }
  }
</style>

  <script>
    async function quickchart(key) {
      const quickchartButtonEl =
        document.querySelector('#' + key + ' button');
      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.
      quickchartButtonEl.classList.add('colab-df-spinner');
      try {
        const charts = await google.colab.kernel.invokeFunction(
            'suggestCharts', [key], {});
      } catch (error) {
        console.error('Error during call to suggestCharts:', error);
      }
      quickchartButtonEl.classList.remove('colab-df-spinner');
      quickchartButtonEl.classList.add('colab-df-quickchart-complete');
    }
    (() => {
      let quickchartButtonEl =
        document.querySelector('#df-4548a829-56fe-48db-a68d-239dc03eb36a button');
      quickchartButtonEl.style.display =
        google.colab.kernel.accessAllowed ? 'block' : 'none';
    })();
  </script>
</div>

    </div>
  </div>

</div>
</div>
<div class="cell markdown" id="F-c7UYxlNY5h">
<p>Data visualizations to get an insight about the dataset.</p>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:1000}"
id="7IMVwf25nra-" data-outputId="3ad4b8f9-2e65-478c-c508-ade46d04ccc4">
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Selecting the columns that are float and int</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>numeric_columns <span class="op">=</span> df.select_dtypes(include<span class="op">=</span>[<span class="st">&#39;float64&#39;</span>, <span class="st">&#39;int64&#39;</span>]).columns</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co"># descibing the plot size</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">20</span>, <span class="bu">len</span>(numeric_columns) <span class="op">*</span> <span class="dv">6</span>))</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Loop through the columns and plot the data of that particular column</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, col <span class="kw">in</span> <span class="bu">enumerate</span>(numeric_columns):</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    plt.subplot(<span class="bu">len</span>(numeric_columns), <span class="dv">2</span>, <span class="dv">2</span><span class="op">*</span>i<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    sns.histplot(df[col], kde<span class="op">=</span><span class="va">True</span>, bins<span class="op">=</span><span class="dv">30</span>, color<span class="op">=</span><span class="st">&#39;blue&#39;</span>) <span class="co"># histogram plot</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="ss">f&#39;Histogram of </span><span class="sc">{</span>col<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    plt.subplot(<span class="bu">len</span>(numeric_columns), <span class="dv">2</span>, <span class="dv">2</span><span class="op">*</span>i<span class="op">+</span><span class="dv">2</span>)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>    sns.boxplot(x<span class="op">=</span>df[col], color<span class="op">=</span><span class="st">&#39;green&#39;</span>) <span class="co"># box plot</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="ss">f&#39;Boxplot of </span><span class="sc">{</span>col<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<div class="output display_data">
<p><img
src="vertopal_901a6a6b87fa426b8677513e8d2b1791/d44df5e1d2dc187a87644c8a1571c2f73746a3e7.png" /></p>
</div>
</div>
<div class="cell markdown" id="bBs_MjnsP3Wa">
<p>Correlational analysis on the dataset and the effect on feature
selection</p>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:930}"
id="1AUPr1I7o1qf" data-outputId="5d25486e-3167-41ff-a34a-18e1661f78a1">
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Checking the correaltion matrix</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>correlation_matrix <span class="op">=</span> df.corr()</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">20</span>, <span class="dv">15</span>))</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>sns.heatmap(correlation_matrix, annot<span class="op">=</span><span class="va">True</span>, cmap<span class="op">=</span><span class="st">&#39;coolwarm&#39;</span>, fmt<span class="op">=</span><span class="st">&quot;.2f&quot;</span>, linewidths<span class="op">=</span><span class="fl">0.5</span>) <span class="co"># heatmap with cool warm colur for visulization</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&#39;Correlation Matrix Heatmap&#39;</span>, fontsize<span class="op">=</span><span class="dv">18</span>)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<div class="output display_data">
<p><img
src="vertopal_901a6a6b87fa426b8677513e8d2b1791/a86324e0098b24d4fe61fbb5b001c1260afdaa67.png" /></p>
</div>
</div>
<div class="cell markdown" id="sWEBz9_1sq25">
<p>Correlation analysis plays an important role in feature selection in
the next step of your machine learning pipeline. This is becuase</p>
<ul>
<li>Highly correlated features often carry redundant information. If two
features have a high correlation, they provide similar predictive power,
so you may want to remove one of them to reduce multicollinearity.</li>
<li>By looking at the correlation between features and the target
variable (if included), you can identify which features are more
important for predicting the target. Features with a higher correlation
to the target variable are more likely to contribute to the modelâ€™s
predictive power.</li>
</ul>
<p>For example, age and years of experience has a correlation of 0.9,
hence it might be better to remove one of them.</p>
</div>
<section id="3-data-pre-processing-and-cleaning" class="cell markdown"
id="nd55cdU-uvbR">
<h1>3. Data Pre-processing and cleaning</h1>
</section>
<div class="cell markdown" id="DgLcwc1GQDVL">
<p>Dataset Before Pre-Processing and Cleaning</p>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:226}"
id="HL2PNxqG2edh" data-outputId="68c610b9-e942-4a2d-9f99-5ab677a34206">
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># print first 5 rows of data</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>df.head(<span class="dv">5</span>)</span></code></pre></div>
<div class="output execute_result" data-execution_count="46">

  <div id="df-ccf805b8-ee11-480d-b91b-49496ba6f6ac" class="colab-df-container">
    <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Customer_ID</th>
      <th>Age</th>
      <th>Years_Experience</th>
      <th>Annual_Income</th>
      <th>ZIP_Code</th>
      <th>Family_size</th>
      <th>Avg_Spending</th>
      <th>Education_Level</th>
      <th>Mortgage</th>
      <th>Has_Consumer_Loan</th>
      <th>Has_Securities_Account</th>
      <th>Has_CD_Account</th>
      <th>Uses_Online_Banking</th>
      <th>Has_CreditCard</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>25</td>
      <td>1</td>
      <td>49</td>
      <td>91107</td>
      <td>4</td>
      <td>1.6</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>45</td>
      <td>19</td>
      <td>34</td>
      <td>90089</td>
      <td>3</td>
      <td>1.5</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>39</td>
      <td>15</td>
      <td>11</td>
      <td>94720</td>
      <td>1</td>
      <td>1.0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>35</td>
      <td>9</td>
      <td>100</td>
      <td>94112</td>
      <td>1</td>
      <td>2.7</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>35</td>
      <td>8</td>
      <td>45</td>
      <td>91330</td>
      <td>4</td>
      <td>1.0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>
    <div class="colab-df-buttons">

  <div class="colab-df-container">
    <button class="colab-df-convert" onclick="convertToInteractive('df-ccf805b8-ee11-480d-b91b-49496ba6f6ac')"
            title="Convert this dataframe to an interactive table."
            style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px" viewBox="0 -960 960 960">
    <path d="M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z"/>
  </svg>
    </button>

  <style>
    .colab-df-container {
      display:flex;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    .colab-df-buttons div {
      margin-bottom: 4px;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

    <script>
      const buttonEl =
        document.querySelector('#df-ccf805b8-ee11-480d-b91b-49496ba6f6ac button.colab-df-convert');
      buttonEl.style.display =
        google.colab.kernel.accessAllowed ? 'block' : 'none';

      async function convertToInteractive(key) {
        const element = document.querySelector('#df-ccf805b8-ee11-480d-b91b-49496ba6f6ac');
        const dataTable =
          await google.colab.kernel.invokeFunction('convertToInteractive',
                                                    [key], {});
        if (!dataTable) return;

        const docLinkHtml = 'Like what you see? Visit the ' +
          '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
          + ' to learn more about interactive tables.';
        element.innerHTML = '';
        dataTable['output_type'] = 'display_data';
        await google.colab.output.renderOutput(dataTable, element);
        const docLink = document.createElement('div');
        docLink.innerHTML = docLinkHtml;
        element.appendChild(docLink);
      }
    </script>
  </div>


<div id="df-ec9e2c6d-7b4b-4963-8686-171e7307b64f">
  <button class="colab-df-quickchart" onclick="quickchart('df-ec9e2c6d-7b4b-4963-8686-171e7307b64f')"
            title="Suggest charts"
            style="display:none;">

<svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
     width="24px">
    <g>
        <path d="M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z"/>
    </g>
</svg>
  </button>

<style>
  .colab-df-quickchart {
      --bg-color: #E8F0FE;
      --fill-color: #1967D2;
      --hover-bg-color: #E2EBFA;
      --hover-fill-color: #174EA6;
      --disabled-fill-color: #AAA;
      --disabled-bg-color: #DDD;
  }

  [theme=dark] .colab-df-quickchart {
      --bg-color: #3B4455;
      --fill-color: #D2E3FC;
      --hover-bg-color: #434B5C;
      --hover-fill-color: #FFFFFF;
      --disabled-bg-color: #3B4455;
      --disabled-fill-color: #666;
  }

  .colab-df-quickchart {
    background-color: var(--bg-color);
    border: none;
    border-radius: 50%;
    cursor: pointer;
    display: none;
    fill: var(--fill-color);
    height: 32px;
    padding: 0;
    width: 32px;
  }

  .colab-df-quickchart:hover {
    background-color: var(--hover-bg-color);
    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);
    fill: var(--button-hover-fill-color);
  }

  .colab-df-quickchart-complete:disabled,
  .colab-df-quickchart-complete:disabled:hover {
    background-color: var(--disabled-bg-color);
    fill: var(--disabled-fill-color);
    box-shadow: none;
  }

  .colab-df-spinner {
    border: 2px solid var(--fill-color);
    border-color: transparent;
    border-bottom-color: var(--fill-color);
    animation:
      spin 1s steps(1) infinite;
  }

  @keyframes spin {
    0% {
      border-color: transparent;
      border-bottom-color: var(--fill-color);
      border-left-color: var(--fill-color);
    }
    20% {
      border-color: transparent;
      border-left-color: var(--fill-color);
      border-top-color: var(--fill-color);
    }
    30% {
      border-color: transparent;
      border-left-color: var(--fill-color);
      border-top-color: var(--fill-color);
      border-right-color: var(--fill-color);
    }
    40% {
      border-color: transparent;
      border-right-color: var(--fill-color);
      border-top-color: var(--fill-color);
    }
    60% {
      border-color: transparent;
      border-right-color: var(--fill-color);
    }
    80% {
      border-color: transparent;
      border-right-color: var(--fill-color);
      border-bottom-color: var(--fill-color);
    }
    90% {
      border-color: transparent;
      border-bottom-color: var(--fill-color);
    }
  }
</style>

  <script>
    async function quickchart(key) {
      const quickchartButtonEl =
        document.querySelector('#' + key + ' button');
      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.
      quickchartButtonEl.classList.add('colab-df-spinner');
      try {
        const charts = await google.colab.kernel.invokeFunction(
            'suggestCharts', [key], {});
      } catch (error) {
        console.error('Error during call to suggestCharts:', error);
      }
      quickchartButtonEl.classList.remove('colab-df-spinner');
      quickchartButtonEl.classList.add('colab-df-quickchart-complete');
    }
    (() => {
      let quickchartButtonEl =
        document.querySelector('#df-ec9e2c6d-7b4b-4963-8686-171e7307b64f button');
      quickchartButtonEl.style.display =
        google.colab.kernel.accessAllowed ? 'block' : 'none';
    })();
  </script>
</div>

    </div>
  </div>

</div>
</div>
<div class="cell markdown" id="pEXKlEShQHws">
<p>Pre-processing steps performed like identifying NULL or Missing
Values if any, handling of outliers if present in the dataset, skewed
data etc.</p>
</div>
<div class="cell markdown" id="KrF8edJhQTMQ">
<p><strong>Pre-Processing Steps:</strong></p>
<ul>
<li>Checking for missing or Null values: None were found in the
dataset.</li>
<li>Checking for Outliers in the dataset. Used the interquartile range
to define the normal range. Any value falling outside this range, is an
outlier. The acceptable range is between Q1 - 1.5 x IQR (lower_bound)
and Q3 + 1.5 x IQR (upper_bound) where IQR = Q3 - Q1 and Q3 is the 75th
percentile and Q1 is the 25th percentile</li>
<li>Checking for columns with skewed data</li>
<li>Dropping Columns such as Customer_ID as it's unique and wouldn't
help improve the model</li>
</ul>
<p>The following <strong>feature engineering techniques</strong> were
used.</p>
<ul>
<li>Columns with large ranges were <strong>normalized</strong> using the
<strong>Min-Max Normalization</strong> method to a range of 0-1. This
Ensures that features contribute equally to the model and prevents
features with large ranges from dominating the learning process, and
improves the overall performance of the model</li>
<li>Columns with high skewness were <strong>transformed</strong> using
the <strong>Log Transformation</strong> method to reduce skewness. This
mproves the normality of the data, which can help linear models and
other algorithms that assume normally distributed features.</li>
</ul>
<p><strong>Feature Selection Methods</strong></p>
<ul>
<li>Using the correlation analysis. Highly correlated features often
carry redundant information. If two features have a high correlation,
they provide similar predictive power, hence one needs to be removed to
reduce multicollinearity. For example, age and years of experience has a
correlation of 0.9, hence the column-age will be removed.</li>
<li>Using Anove F-value to rank the columns based on the most important
ones. The ANOVA F-value is used to assess the relevance of each feature
with respect to the target variable and pick the top N columns, N can be
configurable.</li>
</ul>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="g_TVvz94tI92" data-outputId="0e494689-1041-4f43-aa04-9dd8413d3724">
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Code Cell Description: To detect columns with high number of missing values</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>missing_values_count <span class="op">=</span> df.isnull().<span class="bu">sum</span>()</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>total_rows <span class="op">=</span> <span class="bu">len</span>(df)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>missing_values_percentage <span class="op">=</span> (missing_values_count <span class="op">/</span> total_rows) <span class="op">*</span> <span class="dv">100</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>missing_summary <span class="op">=</span> pd.DataFrame({</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;Missing Values Count&#39;</span>: missing_values_count,</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;Missing Values Percentage&#39;</span>: missing_values_percentage</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="co"># missing_summary = missing_summary[missing_summary[&#39;Missing Values Count&#39;] &gt; 0]</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>missing_summary <span class="op">=</span> missing_summary.sort_values(by<span class="op">=</span><span class="st">&#39;Missing Values Percentage&#39;</span>, ascending<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(missing_summary)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>                         Missing Values Count  Missing Values Percentage
Customer_ID                                 0                        0.0
Age                                         0                        0.0
Years_Experience                            0                        0.0
Annual_Income                               0                        0.0
ZIP_Code                                    0                        0.0
Family_size                                 0                        0.0
Avg_Spending                                0                        0.0
Education_Level                             0                        0.0
Mortgage                                    0                        0.0
Has_Consumer_Loan                           0                        0.0
Has_Securities_Account                      0                        0.0
Has_CD_Account                              0                        0.0
Uses_Online_Banking                         0                        0.0
Has_CreditCard                              0                        0.0
</code></pre>
</div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="1rfvCmYNuq6f" data-outputId="23c73bac-0711-4033-c428-1f1db99d99fe">
<div class="sourceCode" id="cb9"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Code Cell Description: To identify and remove instances that have a high number of outliers</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="co"># function to identify the number of outliers</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> identify_outliers(column):</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    Q1 <span class="op">=</span> column.quantile(<span class="fl">0.25</span>) <span class="co"># Q1</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    Q3 <span class="op">=</span> column.quantile(<span class="fl">0.75</span>) <span class="co"># Q3</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>    IQR <span class="op">=</span> Q3 <span class="op">-</span> Q1 <span class="co"># inter quartile range</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>    lower_bound <span class="op">=</span> Q1 <span class="op">-</span> <span class="fl">1.5</span> <span class="op">*</span> IQR</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    upper_bound <span class="op">=</span> Q3 <span class="op">+</span> <span class="fl">1.5</span> <span class="op">*</span> IQR</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>    outliers <span class="op">=</span> (column <span class="op">&lt;</span> lower_bound) <span class="op">|</span> (column <span class="op">&gt;</span> upper_bound)</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> lower_bound, upper_bound, outliers.<span class="bu">sum</span>()  <span class="co"># Return bounds and count of outliers</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a><span class="co"># function to remove outliers</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> remove_outliers(df, numeric_columns):</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> col <span class="kw">in</span> numeric_columns:</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>        lower_bound, upper_bound, _ <span class="op">=</span> identify_outliers(df[col])</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>        df <span class="op">=</span> df[(df[col] <span class="op">&gt;=</span> lower_bound) <span class="op">&amp;</span> (df[col] <span class="op">&lt;=</span> upper_bound)]</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> df</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Selecting the columns that are float and int</span></span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>numeric_columns <span class="op">=</span> df.select_dtypes(include<span class="op">=</span>[<span class="st">&#39;float64&#39;</span>, <span class="st">&#39;int64&#39;</span>]).columns</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>outliers_count <span class="op">=</span> {}</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a><span class="co"># loop through the columns to identify outliers</span></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> col <span class="kw">in</span> numeric_columns:</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>    _, _, count <span class="op">=</span> identify_outliers(df[col])</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>    outliers_count[col] <span class="op">=</span> count</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a><span class="co"># dataframe of outliers from diff columns</span></span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>outliers_summary <span class="op">=</span> pd.DataFrame(<span class="bu">list</span>(outliers_count.items()), columns<span class="op">=</span>[<span class="st">&#39;Column&#39;</span>, <span class="st">&#39;Outliers Count&#39;</span>])</span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>high_outliers <span class="op">=</span> outliers_summary[outliers_summary[<span class="st">&#39;Outliers Count&#39;</span>] <span class="op">&gt;</span> <span class="dv">5</span>]</span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;All Outliers Summary:&quot;</span>)</span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(outliers_summary)</span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Columns with High Number of Outliers:&quot;</span>)</span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(high_outliers)</span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> remove_outliers(df, numeric_columns)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>All Outliers Summary:
                     Column  Outliers Count
0               Customer_ID               0
1                       Age               0
2         Years_Experience                0
3            Annual_Income               96
4                  ZIP_Code               1
5               Family_size               0
6              Avg_Spending             324
7           Education_Level               0
8                  Mortgage             291
9         Has_Consumer_Loan             480
10  Has_Securities_Account              522
11          Has_CD_Account              302
12     Uses_Online_Banking                0
13          Has_CreditCard                0

Columns with High Number of Outliers:
                     Column  Outliers Count
3            Annual_Income               96
6              Avg_Spending             324
8                  Mortgage             291
9         Has_Consumer_Loan             480
10  Has_Securities_Account              522
11          Has_CD_Account              302
</code></pre>
</div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="YAq-LWrpz49E" data-outputId="a581a590-db5d-4048-f4c2-5b3183766d06">
<div class="sourceCode" id="cb11"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Code Cell Description: To detect columns that have high ranges so that they can be normalized. And to detect columns with high skew so that they can be handled.</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="co"># function to detect if any column requires to be normalized</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> detect_normalization_needed(df):</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    numeric_columns <span class="op">=</span> df.select_dtypes(include<span class="op">=</span>[<span class="st">&#39;float64&#39;</span>, <span class="st">&#39;int64&#39;</span>]).columns</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    skewness <span class="op">=</span> df[numeric_columns].<span class="bu">apply</span>(<span class="kw">lambda</span> x: skew(x.dropna())) <span class="co"># checking skewness (Standard Scalar)</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>    high_skew_columns <span class="op">=</span> skewness[<span class="bu">abs</span>(skewness) <span class="op">&gt;</span> <span class="dv">1</span>].index</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>    std_dev <span class="op">=</span> df[numeric_columns].std() <span class="co"># checking standard deviation for high range columns (min max scalar)</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>    high_range_columns <span class="op">=</span> std_dev[std_dev <span class="op">&gt;</span> <span class="dv">1</span>].index</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> high_skew_columns, high_range_columns</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a><span class="co"># check and get columns for normalizations</span></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>high_skew_columns, high_range_columns <span class="op">=</span> detect_normalization_needed(df)</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a><span class="co"># defining normalization Techniques</span></span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>min_max_scaler <span class="op">=</span> MinMaxScaler()</span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Normalizig the columns</span></span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> col <span class="kw">in</span> high_skew_columns:</span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>    df[col] <span class="op">=</span> np.log1p(df[col])</span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> col <span class="kw">in</span> high_range_columns:</span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>    df[[col]] <span class="op">=</span> min_max_scaler.fit_transform(df[[col]])</span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Columns with high skewness transformed:&quot;</span>, high_skew_columns)</span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Columns with large range normalized:&quot;</span>, high_range_columns)</span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df.drop([<span class="st">&quot;Age&quot;</span>,<span class="st">&quot;Customer_ID&quot;</span>,<span class="st">&quot;ZIP_Code&quot;</span>], axis<span class="op">=</span><span class="dv">1</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Columns with high skewness transformed: Index([&#39;Mortgage&#39;], dtype=&#39;object&#39;)
Columns with large range normalized: Index([&#39;Customer_ID&#39;, &#39;Age&#39;, &#39;Years_Experience &#39;, &#39;Annual_Income &#39;, &#39;ZIP_Code&#39;,
       &#39;Family_size&#39;, &#39;Avg_Spending&#39;, &#39;Mortgage&#39;],
      dtype=&#39;object&#39;)
</code></pre>
</div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="PdvltB6-n4Vc" data-outputId="7daeb320-c7d7-478d-b8da-8c0002aba4ea">
<div class="sourceCode" id="cb13"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Code Cell Description: Using Anove F-value to detect most prominent columns</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="co"># defining input and output data</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df.drop(columns<span class="op">=</span>[<span class="st">&#39;Has_CreditCard &#39;</span>]) <span class="co"># input data</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df[<span class="st">&#39;Has_CreditCard &#39;</span>] <span class="co"># output label</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> X.isnull().<span class="bu">sum</span>().<span class="bu">sum</span>() <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;Warning: Missing values found in the DataFrame. Consider imputing or dropping them.&quot;</span>)</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>numeric_columns <span class="op">=</span> X.select_dtypes(include<span class="op">=</span>[<span class="st">&#39;float64&#39;</span>, <span class="st">&#39;int64&#39;</span>]).columns</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="bu">len</span>(numeric_columns) <span class="op">!=</span> X.shape[<span class="dv">1</span>]:</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;Warning: Non-numeric columns found. Ensure all features are numeric for ANOVA.&quot;</span>)</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a><span class="co"># selecting the most prominent features</span></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>selector <span class="op">=</span> SelectKBest(score_func<span class="op">=</span>f_classif, k<span class="op">=</span><span class="st">&#39;all&#39;</span>)</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>selector.fit(X, y)</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>feature_scores <span class="op">=</span> selector.scores_</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>p_values <span class="op">=</span> selector.pvalues_</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>features_summary <span class="op">=</span> pd.DataFrame({</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;Feature&#39;</span>: X.columns,</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;F-Value&#39;</span>: feature_scores,</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;P-Value&#39;</span>: p_values</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>}).sort_values(by<span class="op">=</span><span class="st">&#39;F-Value&#39;</span>, ascending<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Feature Scores and P-Values:&quot;</span>)</span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(features_summary)</span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>num_features_to_select <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>top_features <span class="op">=</span> features_summary.head(num_features_to_select)[<span class="st">&#39;Feature&#39;</span>]</span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> X[top_features]</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Feature Scores and P-Values:
                   Feature   F-Value   P-Value
4          Education_Level  1.674502  0.195739
9     Uses_Online_Banking   0.774571  0.378865
1           Annual_Income   0.422054  0.515955
2              Family_size  0.362410  0.547208
0        Years_Experience   0.288985  0.590905
5                 Mortgage  0.026603  0.870446
3             Avg_Spending  0.002401  0.960922
6        Has_Consumer_Loan       NaN       NaN
7  Has_Securities_Account        NaN       NaN
8          Has_CD_Account        NaN       NaN
</code></pre>
</div>
</div>
<div class="cell markdown" id="IowhE3mBa05v">
<p>Dataset after pre-processing and cleaning</p>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:206}"
id="RRciNO4M1E1S" data-outputId="32d11465-64a2-43d7-b728-da9de83074dc">
<div class="sourceCode" id="cb15"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>df.head(<span class="dv">5</span>)</span></code></pre></div>
<div class="output execute_result" data-execution_count="51">

  <div id="df-0efef3f2-d8a8-44b2-a5c6-871a330aabba" class="colab-df-container">
    <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Education_Level</th>
      <th>Uses_Online_Banking</th>
      <th>Annual_Income</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>0</td>
      <td>0.016949</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2</td>
      <td>0</td>
      <td>0.519774</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2</td>
      <td>0</td>
      <td>0.209040</td>
    </tr>
    <tr>
      <th>5</th>
      <td>2</td>
      <td>1</td>
      <td>0.118644</td>
    </tr>
    <tr>
      <th>6</th>
      <td>2</td>
      <td>1</td>
      <td>0.361582</td>
    </tr>
  </tbody>
</table>
</div>
    <div class="colab-df-buttons">

  <div class="colab-df-container">
    <button class="colab-df-convert" onclick="convertToInteractive('df-0efef3f2-d8a8-44b2-a5c6-871a330aabba')"
            title="Convert this dataframe to an interactive table."
            style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px" viewBox="0 -960 960 960">
    <path d="M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z"/>
  </svg>
    </button>

  <style>
    .colab-df-container {
      display:flex;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    .colab-df-buttons div {
      margin-bottom: 4px;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

    <script>
      const buttonEl =
        document.querySelector('#df-0efef3f2-d8a8-44b2-a5c6-871a330aabba button.colab-df-convert');
      buttonEl.style.display =
        google.colab.kernel.accessAllowed ? 'block' : 'none';

      async function convertToInteractive(key) {
        const element = document.querySelector('#df-0efef3f2-d8a8-44b2-a5c6-871a330aabba');
        const dataTable =
          await google.colab.kernel.invokeFunction('convertToInteractive',
                                                    [key], {});
        if (!dataTable) return;

        const docLinkHtml = 'Like what you see? Visit the ' +
          '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
          + ' to learn more about interactive tables.';
        element.innerHTML = '';
        dataTable['output_type'] = 'display_data';
        await google.colab.output.renderOutput(dataTable, element);
        const docLink = document.createElement('div');
        docLink.innerHTML = docLinkHtml;
        element.appendChild(docLink);
      }
    </script>
  </div>


<div id="df-e8a7f081-49e4-46b1-87ed-8dbf45cb6ac9">
  <button class="colab-df-quickchart" onclick="quickchart('df-e8a7f081-49e4-46b1-87ed-8dbf45cb6ac9')"
            title="Suggest charts"
            style="display:none;">

<svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
     width="24px">
    <g>
        <path d="M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z"/>
    </g>
</svg>
  </button>

<style>
  .colab-df-quickchart {
      --bg-color: #E8F0FE;
      --fill-color: #1967D2;
      --hover-bg-color: #E2EBFA;
      --hover-fill-color: #174EA6;
      --disabled-fill-color: #AAA;
      --disabled-bg-color: #DDD;
  }

  [theme=dark] .colab-df-quickchart {
      --bg-color: #3B4455;
      --fill-color: #D2E3FC;
      --hover-bg-color: #434B5C;
      --hover-fill-color: #FFFFFF;
      --disabled-bg-color: #3B4455;
      --disabled-fill-color: #666;
  }

  .colab-df-quickchart {
    background-color: var(--bg-color);
    border: none;
    border-radius: 50%;
    cursor: pointer;
    display: none;
    fill: var(--fill-color);
    height: 32px;
    padding: 0;
    width: 32px;
  }

  .colab-df-quickchart:hover {
    background-color: var(--hover-bg-color);
    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);
    fill: var(--button-hover-fill-color);
  }

  .colab-df-quickchart-complete:disabled,
  .colab-df-quickchart-complete:disabled:hover {
    background-color: var(--disabled-bg-color);
    fill: var(--disabled-fill-color);
    box-shadow: none;
  }

  .colab-df-spinner {
    border: 2px solid var(--fill-color);
    border-color: transparent;
    border-bottom-color: var(--fill-color);
    animation:
      spin 1s steps(1) infinite;
  }

  @keyframes spin {
    0% {
      border-color: transparent;
      border-bottom-color: var(--fill-color);
      border-left-color: var(--fill-color);
    }
    20% {
      border-color: transparent;
      border-left-color: var(--fill-color);
      border-top-color: var(--fill-color);
    }
    30% {
      border-color: transparent;
      border-left-color: var(--fill-color);
      border-top-color: var(--fill-color);
      border-right-color: var(--fill-color);
    }
    40% {
      border-color: transparent;
      border-right-color: var(--fill-color);
      border-top-color: var(--fill-color);
    }
    60% {
      border-color: transparent;
      border-right-color: var(--fill-color);
    }
    80% {
      border-color: transparent;
      border-right-color: var(--fill-color);
      border-bottom-color: var(--fill-color);
    }
    90% {
      border-color: transparent;
      border-bottom-color: var(--fill-color);
    }
  }
</style>

  <script>
    async function quickchart(key) {
      const quickchartButtonEl =
        document.querySelector('#' + key + ' button');
      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.
      quickchartButtonEl.classList.add('colab-df-spinner');
      try {
        const charts = await google.colab.kernel.invokeFunction(
            'suggestCharts', [key], {});
      } catch (error) {
        console.error('Error during call to suggestCharts:', error);
      }
      quickchartButtonEl.classList.remove('colab-df-spinner');
      quickchartButtonEl.classList.add('colab-df-quickchart-complete');
    }
    (() => {
      let quickchartButtonEl =
        document.querySelector('#df-e8a7f081-49e4-46b1-87ed-8dbf45cb6ac9 button');
      quickchartButtonEl.style.display =
        google.colab.kernel.accessAllowed ? 'block' : 'none';
    })();
  </script>
</div>

    </div>
  </div>

</div>
</div>
<section id="4-model-building" class="cell markdown" id="PX4KERZL9Z6h">
<h1>4. Model Building</h1>
</section>
<div class="cell markdown" id="bGw8eeApRyqt">
<p>The primary purpose of splitting the data is to evaluate the
performance of the model. The training set is used to fit the model,
while the test set is used to assess how well the model generalizes to
new data.</p>
<p>By keeping a separate test set,the model's performance is not just a
result of memorizing the training data but can be generalized. This
prevents overfitting.</p>
<p>It also helps in paramerter tuning and comparing the performance of
multiple models.</p>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:144}"
id="caRWQKwPZoXb" data-outputId="241d121e-2081-4a65-ab44-93a0805ad09d">
<div class="sourceCode" id="cb16"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Code Cell Description: Building Logistic Regression and Decision Tree Models. Grid Search is used to find optimal parameter combination.</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>) <span class="co"># split the data into traina and test (80: 20)</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>log_reg <span class="op">=</span> LogisticRegression(max_iter<span class="op">=</span><span class="dv">200</span>, random_state<span class="op">=</span><span class="dv">42</span>) <span class="co"># define logistic regression</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>dt <span class="op">=</span> DecisionTreeClassifier(random_state<span class="op">=</span><span class="dv">42</span>) <span class="co"># define Decision Tree</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a><span class="co"># parameters for hyper parameter tuning</span></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>param_grid <span class="op">=</span> {</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;criterion&#39;</span>: [<span class="st">&#39;gini&#39;</span>, <span class="st">&#39;entropy&#39;</span>],</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;max_depth&#39;</span>: [<span class="va">None</span>, <span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">30</span>],</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;min_samples_split&#39;</span>: [<span class="dv">2</span>, <span class="dv">5</span>, <span class="dv">10</span>],</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;min_samples_leaf&#39;</span>: [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">5</span>]</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Grid search to find best hyper parameters for Decision tree</span></span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>grid_search <span class="op">=</span> GridSearchCV(estimator<span class="op">=</span>dt, param_grid<span class="op">=</span>param_grid, cv<span class="op">=</span><span class="dv">5</span>, n_jobs<span class="op">=-</span><span class="dv">1</span>, verbose<span class="op">=</span><span class="dv">1</span>, scoring<span class="op">=</span><span class="st">&#39;accuracy&#39;</span>)</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>grid_search.fit(X_train, y_train) <span class="co"># Grid search finds best hyper parameters on train data</span></span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a><span class="co"># results of hyper parameter tuning</span></span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>best_dt <span class="op">=</span> grid_search.best_estimator_</span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>best_params <span class="op">=</span> grid_search.best_params_</span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Best Parameters for Decision Tree:&quot;</span>, best_params,<span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a><span class="co"># define scoring paramters (weighted average method - to handle class imbalance)</span></span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a>scoring <span class="op">=</span> {</span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;accuracy&#39;</span>: make_scorer(accuracy_score),</span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;f1&#39;</span>: make_scorer(f1_score, average<span class="op">=</span><span class="st">&#39;weighted&#39;</span>)</span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true" tabindex="-1"></a><span class="co"># 5 fold cross validation of train data for Logistic regression</span></span>
<span id="cb16-31"><a href="#cb16-31" aria-hidden="true" tabindex="-1"></a>log_reg_accuracy <span class="op">=</span> cross_val_score(log_reg, X_train, y_train, cv<span class="op">=</span><span class="dv">5</span>, scoring<span class="op">=</span><span class="st">&#39;accuracy&#39;</span>)</span>
<span id="cb16-32"><a href="#cb16-32" aria-hidden="true" tabindex="-1"></a>log_reg_f1 <span class="op">=</span> cross_val_score(log_reg, X_train, y_train, cv<span class="op">=</span><span class="dv">5</span>, scoring<span class="op">=</span><span class="st">&#39;f1_weighted&#39;</span>)</span>
<span id="cb16-33"><a href="#cb16-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-34"><a href="#cb16-34" aria-hidden="true" tabindex="-1"></a><span class="co"># 5 fold cross validation of train data for Decision Tree using results of hyper parameter tuning</span></span>
<span id="cb16-35"><a href="#cb16-35" aria-hidden="true" tabindex="-1"></a>best_dt_accuracy <span class="op">=</span> cross_val_score(best_dt, X_train, y_train, cv<span class="op">=</span><span class="dv">5</span>, scoring<span class="op">=</span><span class="st">&#39;accuracy&#39;</span>)</span>
<span id="cb16-36"><a href="#cb16-36" aria-hidden="true" tabindex="-1"></a>best_dt_f1 <span class="op">=</span> cross_val_score(best_dt, X_train, y_train, cv<span class="op">=</span><span class="dv">5</span>, scoring<span class="op">=</span><span class="st">&#39;f1_weighted&#39;</span>)</span>
<span id="cb16-37"><a href="#cb16-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-38"><a href="#cb16-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Training Logistic regreesion and decision Tree on train data</span></span>
<span id="cb16-39"><a href="#cb16-39" aria-hidden="true" tabindex="-1"></a>log_reg.fit(X_train, y_train)</span>
<span id="cb16-40"><a href="#cb16-40" aria-hidden="true" tabindex="-1"></a>best_dt.fit(X_train, y_train)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Fitting 5 folds for each of 72 candidates, totalling 360 fits
Best Parameters for Decision Tree: {&#39;criterion&#39;: &#39;entropy&#39;, &#39;max_depth&#39;: 10, &#39;min_samples_leaf&#39;: 2, &#39;min_samples_split&#39;: 2} 

</code></pre>
</div>
<div class="output execute_result" data-execution_count="52">
<style>#sk-container-id-7 {color: black;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: "â–¸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "â–¾";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-7" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>DecisionTreeClassifier(criterion=&#x27;entropy&#x27;, max_depth=10, min_samples_leaf=2,
                       random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-7" type="checkbox" checked><label for="sk-estimator-id-7" class="sk-toggleable__label sk-toggleable__label-arrow">DecisionTreeClassifier</label><div class="sk-toggleable__content"><pre>DecisionTreeClassifier(criterion=&#x27;entropy&#x27;, max_depth=10, min_samples_leaf=2,
                       random_state=42)</pre></div></div></div></div></div>
</div>
</div>
<div class="cell markdown" id="3hZ7Xu05a88A">
<p>The Ideal conditions for the decision tree was found to be as
below</p>
<ul>
<li>criterion='entropy': Uses entropy to measure the quality of a
split.</li>
<li>max_depth=10: Limits the maximum depth of the tree to 10
levels.</li>
<li>min_samples_leaf=1: Specifies the minimum number of samples required
to be at a leaf node.</li>
<li>min_samples_split=5: Specifies the minimum number of samples
required to split an internal node.</li>
</ul>
<p>These conditions were chosen because this combination yielded the
best results after performing cross validation to evaluate the model's
performance. The combination of hyperparameters that yields the best
average performance across the folds is chosen as the optimal set.</p>
</div>
<div class="cell markdown" id="55nNLo6naDqi">
<p>Running the same code but with different train-test-split ration
(70-30)</p>
</div>
<section id="5-performance-evaluation" class="cell markdown"
id="1_yuR6vIcnGi">
<h1>5. Performance Evaluation</h1>
</section>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="DG8miu24cweC" data-outputId="ee1c4f48-f7f6-4422-849e-9a375f91aa4f">
<div class="sourceCode" id="cb18"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Code Cell Description: Compare Performance of Logisitc Regression and Decision Tree models.</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Getting the results of Trained Models on test data</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>log_reg_test_score <span class="op">=</span> log_reg.score(X_test, y_test) <span class="co"># logistic regression results on test data</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>best_dt_test_score <span class="op">=</span> best_dt.score(X_test, y_test) <span class="co"># Decision Tree results on test data</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a><span class="co"># DataFrame summary of performance metrics comparing Logistic Regression and Decision Tree</span></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> pd.DataFrame({</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;Model&#39;</span>: [<span class="st">&#39;Logistic Regression&#39;</span>, <span class="st">&#39;Decision Tree (Best Params)&#39;</span>],</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;CV Accuracy Mean&#39;</span>: [np.mean(log_reg_accuracy), np.mean(best_dt_accuracy)],</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;CV Accuracy Std&#39;</span>: [np.std(log_reg_accuracy), np.std(best_dt_accuracy)],</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;CV F1 Score Mean&#39;</span>: [np.mean(log_reg_f1), np.mean(best_dt_f1)],</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;CV F1 Score Std&#39;</span>: [np.std(log_reg_f1), np.std(best_dt_f1)],</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;Test Accuracy&#39;</span>: [log_reg_test_score, best_dt_test_score]</span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(results)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>                         Model  CV Accuracy Mean  CV Accuracy Std  \
0          Logistic Regression          0.712414         0.000690   
1  Decision Tree (Best Params)          0.674483         0.009018   

   CV F1 Score Mean  CV F1 Score Std  Test Accuracy  
0          0.592770         0.000909       0.703448  
1          0.614874         0.007703       0.662069  
</code></pre>
</div>
</div>
<div class="cell markdown" id="EYxjvd8fvO9M">
<ul>
<li>Logistic Regression has a higher CV Accuracy Mean (0.7124) compared
to the Decision Tree (0.6745). This suggests that on average, Logistic
Regression performs better across different cross-validation splits,
indicating better generalization performance.</li>
<li>The standard deviation of Logistic Regression (0.00069) is much
lower than that of the Decision Tree (0.00902). This means that Logistic
Regression is more consistent in its performance across different
cross-validation splits, whereas the Decision Tree shows more
variability and is less stable.</li>
<li>The F1 Score Mean for the Decision Tree (0.6149) is slightly higher
than for Logistic Regression (0.5928). F1 score balances precision and
recall, so the Decision Tree might handle imbalanced data better or
perform better in terms of balancing false positives and false
negatives</li>
<li>Logistic Regression again outperforms the Decision Tree with a
higher Test Accuracy (0.7034 vs. 0.6621), meaning that Logistic
Regression performs better on the unseen test data</li>
</ul>
<p>Logistic Regression is better overall due to its higher accuracy,
both in cross-validation and on the test set. It also has a lower
standard deviation, meaning it performs more consistently across
different data splits. Decision Tree, while having a slightly better F1
score, is less consistent and has lower overall accuracy. It may be more
suitable if you prioritize balancing precision and recall in scenarios
like imbalanced datasets.</p>
</div>
<section
id="repeating-the-task-with-a-different-train-test-split-ratio-70-30"
class="cell markdown" id="yBlnaJLKc1w9">
<h1>Repeating the task with a different train-test split ratio
(70-30)</h1>
</section>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="60caQRx5ViiT" data-outputId="712b11ed-edb3-4e7c-d46a-12e7b81cf13c">
<div class="sourceCode" id="cb20"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Code Cell Description: Re-training and predicting with new train-test-split ratio (training - 70% and testing - 30%)</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">42</span>) <span class="co"># split the data into traina and test (70: 30)</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>log_reg <span class="op">=</span> LogisticRegression(max_iter<span class="op">=</span><span class="dv">200</span>, random_state<span class="op">=</span><span class="dv">42</span>) <span class="co"># defining Logistic Regression</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>dt <span class="op">=</span> DecisionTreeClassifier(random_state<span class="op">=</span><span class="dv">42</span>) <span class="co"># defining Decision Tree</span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a><span class="co"># defining different parameters of hyper parameter tuning</span></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>param_grid <span class="op">=</span> {</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;criterion&#39;</span>: [<span class="st">&#39;gini&#39;</span>, <span class="st">&#39;entropy&#39;</span>],</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;max_depth&#39;</span>: [<span class="va">None</span>, <span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">30</span>],</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;min_samples_split&#39;</span>: [<span class="dv">2</span>, <span class="dv">5</span>, <span class="dv">10</span>],</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;min_samples_leaf&#39;</span>: [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">5</span>]</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a><span class="co"># grid search to find best hyper parameters for decision Tree</span></span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>grid_search <span class="op">=</span> GridSearchCV(estimator<span class="op">=</span>dt, param_grid<span class="op">=</span>param_grid, cv<span class="op">=</span><span class="dv">5</span>, n_jobs<span class="op">=-</span><span class="dv">1</span>, verbose<span class="op">=</span><span class="dv">1</span>, scoring<span class="op">=</span><span class="st">&#39;accuracy&#39;</span>)</span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a>grid_search.fit(X_train, y_train) <span class="co"># run train data to find best hyper parameters</span></span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a><span class="co"># results of hyper parameter tuning</span></span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a>best_dt <span class="op">=</span> grid_search.best_estimator_</span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a>best_params <span class="op">=</span> grid_search.best_params_</span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Best Parameters for Decision Tree:&quot;</span>, best_params,<span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)</span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a>scoring <span class="op">=</span> {</span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;accuracy&#39;</span>: make_scorer(accuracy_score),</span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;f1&#39;</span>: make_scorer(f1_score, average<span class="op">=</span><span class="st">&#39;weighted&#39;</span>)</span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a><span class="co"># 5 fold cross validation of train data for Logistic regression</span></span>
<span id="cb20-33"><a href="#cb20-33" aria-hidden="true" tabindex="-1"></a>log_reg_accuracy <span class="op">=</span> cross_val_score(log_reg, X_train, y_train, cv<span class="op">=</span><span class="dv">5</span>, scoring<span class="op">=</span><span class="st">&#39;accuracy&#39;</span>)</span>
<span id="cb20-34"><a href="#cb20-34" aria-hidden="true" tabindex="-1"></a>log_reg_f1 <span class="op">=</span> cross_val_score(log_reg, X_train, y_train, cv<span class="op">=</span><span class="dv">5</span>, scoring<span class="op">=</span><span class="st">&#39;f1_weighted&#39;</span>)</span>
<span id="cb20-35"><a href="#cb20-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-36"><a href="#cb20-36" aria-hidden="true" tabindex="-1"></a><span class="co"># 5 fold cross validation of train data for Decision Tree using best hyper parameters</span></span>
<span id="cb20-37"><a href="#cb20-37" aria-hidden="true" tabindex="-1"></a>best_dt_accuracy <span class="op">=</span> cross_val_score(best_dt, X_train, y_train, cv<span class="op">=</span><span class="dv">5</span>, scoring<span class="op">=</span><span class="st">&#39;accuracy&#39;</span>)</span>
<span id="cb20-38"><a href="#cb20-38" aria-hidden="true" tabindex="-1"></a>best_dt_f1 <span class="op">=</span> cross_val_score(best_dt, X_train, y_train, cv<span class="op">=</span><span class="dv">5</span>, scoring<span class="op">=</span><span class="st">&#39;f1_weighted&#39;</span>)</span>
<span id="cb20-39"><a href="#cb20-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-40"><a href="#cb20-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Traing the models on Train Data</span></span>
<span id="cb20-41"><a href="#cb20-41" aria-hidden="true" tabindex="-1"></a>log_reg.fit(X_train, y_train) <span class="co"># Training Logistic Regression</span></span>
<span id="cb20-42"><a href="#cb20-42" aria-hidden="true" tabindex="-1"></a>best_dt.fit(X_train, y_train) <span class="co"># Training Decision Tree</span></span>
<span id="cb20-43"><a href="#cb20-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-44"><a href="#cb20-44" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluating and scoring the model on Test Data</span></span>
<span id="cb20-45"><a href="#cb20-45" aria-hidden="true" tabindex="-1"></a>log_reg_test_score <span class="op">=</span> log_reg.score(X_test, y_test) <span class="co"># Score of Logistic Regression</span></span>
<span id="cb20-46"><a href="#cb20-46" aria-hidden="true" tabindex="-1"></a>best_dt_test_score <span class="op">=</span> best_dt.score(X_test, y_test) <span class="co"># Score of Decision Tree</span></span>
<span id="cb20-47"><a href="#cb20-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-48"><a href="#cb20-48" aria-hidden="true" tabindex="-1"></a><span class="co"># DataFrame summary of performance metrics comparing Logistic Regression and Decision Tree</span></span>
<span id="cb20-49"><a href="#cb20-49" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> pd.DataFrame({</span>
<span id="cb20-50"><a href="#cb20-50" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;Model&#39;</span>: [<span class="st">&#39;Logistic Regression&#39;</span>, <span class="st">&#39;Decision Tree (Best Params)&#39;</span>],</span>
<span id="cb20-51"><a href="#cb20-51" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;CV Accuracy Mean&#39;</span>: [np.mean(log_reg_accuracy), np.mean(best_dt_accuracy)],</span>
<span id="cb20-52"><a href="#cb20-52" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;CV Accuracy Std&#39;</span>: [np.std(log_reg_accuracy), np.std(best_dt_accuracy)],</span>
<span id="cb20-53"><a href="#cb20-53" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;CV F1 Score Mean&#39;</span>: [np.mean(log_reg_f1), np.mean(best_dt_f1)],</span>
<span id="cb20-54"><a href="#cb20-54" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;CV F1 Score Std&#39;</span>: [np.std(log_reg_f1), np.std(best_dt_f1)],</span>
<span id="cb20-55"><a href="#cb20-55" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;Test Accuracy&#39;</span>: [log_reg_test_score, best_dt_test_score]</span>
<span id="cb20-56"><a href="#cb20-56" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb20-57"><a href="#cb20-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-58"><a href="#cb20-58" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(results)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Fitting 5 folds for each of 72 candidates, totalling 360 fits
Best Parameters for Decision Tree: {&#39;criterion&#39;: &#39;entropy&#39;, &#39;max_depth&#39;: 10, &#39;min_samples_leaf&#39;: 1, &#39;min_samples_split&#39;: 5} 

                         Model  CV Accuracy Mean  CV Accuracy Std  \
0          Logistic Regression          0.709500         0.000685   
1  Decision Tree (Best Params)          0.674037         0.014695   

   CV F1 Score Mean  CV F1 Score Std  Test Accuracy  
0          0.588933         0.000901       0.713235  
1          0.621130         0.020374       0.689338  
</code></pre>
</div>
</div>
<section id="assignment-part-2" class="cell markdown" id="7D5icN7Xg9OE">
<h1>Assignment Part 2</h1>
</section>
<section id="1-model-building" class="cell markdown" id="LF2SLC4Ol15G">
<h1>1) Model Building</h1>
</section>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:74}"
id="1ERGcMSug_L-" data-outputId="deac2b7b-d05c-4021-9163-4b4657d0f456">
<div class="sourceCode" id="cb22"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Code Cell Description: Building the model with KNN, Naive Bayes, Random Forest and Adaboost</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="co"># split the data into training and testing in ration of 80:20</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Defining different Alogorithms</span></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>knn <span class="op">=</span> KNeighborsClassifier() <span class="co"># knn algorithm</span></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>nb <span class="op">=</span> GaussianNB() <span class="co"># Naive bayes algorithm</span></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>rf <span class="op">=</span> RandomForestClassifier(random_state<span class="op">=</span><span class="dv">42</span>) <span class="co"># Random forest classifier</span></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>ada <span class="op">=</span> AdaBoostClassifier(random_state<span class="op">=</span><span class="dv">42</span>) <span class="co"># Adaboost Classifier</span></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a><span class="co"># define scoring parameters (performance evaluators)</span></span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>scoring <span class="op">=</span> {</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;accuracy&#39;</span>: make_scorer(accuracy_score),</span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;f1&#39;</span>: make_scorer(f1_score, average<span class="op">=</span><span class="st">&#39;weighted&#39;</span>)</span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a><span class="co"># 5 fold cross validation of train data for KNN</span></span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a>knn_accuracy <span class="op">=</span> cross_val_score(knn, X_train, y_train, cv<span class="op">=</span><span class="dv">5</span>, scoring<span class="op">=</span><span class="st">&#39;accuracy&#39;</span>)</span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a>knn_f1 <span class="op">=</span> cross_val_score(knn, X_train, y_train, cv<span class="op">=</span><span class="dv">5</span>, scoring<span class="op">=</span><span class="st">&#39;f1_weighted&#39;</span>)</span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a><span class="co"># 5 fold cross validation of train data for Naive Bayes</span></span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a>nb_accuracy <span class="op">=</span> cross_val_score(nb, X_train, y_train, cv<span class="op">=</span><span class="dv">5</span>, scoring<span class="op">=</span><span class="st">&#39;accuracy&#39;</span>)</span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a>nb_f1 <span class="op">=</span> cross_val_score(nb, X_train, y_train, cv<span class="op">=</span><span class="dv">5</span>, scoring<span class="op">=</span><span class="st">&#39;f1_weighted&#39;</span>)</span>
<span id="cb22-25"><a href="#cb22-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-26"><a href="#cb22-26" aria-hidden="true" tabindex="-1"></a><span class="co"># 5 fold cross validation of train data for Random forest Classifier</span></span>
<span id="cb22-27"><a href="#cb22-27" aria-hidden="true" tabindex="-1"></a>rf_accuracy <span class="op">=</span> cross_val_score(rf, X_train, y_train, cv<span class="op">=</span><span class="dv">5</span>, scoring<span class="op">=</span><span class="st">&#39;accuracy&#39;</span>)</span>
<span id="cb22-28"><a href="#cb22-28" aria-hidden="true" tabindex="-1"></a>rf_f1 <span class="op">=</span> cross_val_score(rf, X_train, y_train, cv<span class="op">=</span><span class="dv">5</span>, scoring<span class="op">=</span><span class="st">&#39;f1_weighted&#39;</span>)</span>
<span id="cb22-29"><a href="#cb22-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-30"><a href="#cb22-30" aria-hidden="true" tabindex="-1"></a><span class="co"># 5 fold cross validation of train data for Adaboost Classifier</span></span>
<span id="cb22-31"><a href="#cb22-31" aria-hidden="true" tabindex="-1"></a>ada_accuracy <span class="op">=</span> cross_val_score(ada, X_train, y_train, cv<span class="op">=</span><span class="dv">5</span>, scoring<span class="op">=</span><span class="st">&#39;accuracy&#39;</span>)</span>
<span id="cb22-32"><a href="#cb22-32" aria-hidden="true" tabindex="-1"></a>ada_f1 <span class="op">=</span> cross_val_score(ada, X_train, y_train, cv<span class="op">=</span><span class="dv">5</span>, scoring<span class="op">=</span><span class="st">&#39;f1_weighted&#39;</span>)</span>
<span id="cb22-33"><a href="#cb22-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-34"><a href="#cb22-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Training the model</span></span>
<span id="cb22-35"><a href="#cb22-35" aria-hidden="true" tabindex="-1"></a>knn.fit(X_train, y_train) <span class="co"># KNN model Training</span></span>
<span id="cb22-36"><a href="#cb22-36" aria-hidden="true" tabindex="-1"></a>nb.fit(X_train, y_train) <span class="co"># Naive Bayes Training</span></span>
<span id="cb22-37"><a href="#cb22-37" aria-hidden="true" tabindex="-1"></a>rf.fit(X_train, y_train) <span class="co"># Random forest Training</span></span>
<span id="cb22-38"><a href="#cb22-38" aria-hidden="true" tabindex="-1"></a>ada.fit(X_train, y_train) <span class="co"># Adaboost Training</span></span></code></pre></div>
<div class="output execute_result" data-execution_count="55">
<style>#sk-container-id-8 {color: black;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: "â–¸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "â–¾";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-8" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>AdaBoostClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-8" type="checkbox" checked><label for="sk-estimator-id-8" class="sk-toggleable__label sk-toggleable__label-arrow">AdaBoostClassifier</label><div class="sk-toggleable__content"><pre>AdaBoostClassifier(random_state=42)</pre></div></div></div></div></div>
</div>
</div>
<section id="2-performance-evaluation" class="cell markdown"
id="88QtYL5ol4Tf">
<h1>2) Performance Evaluation</h1>
</section>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="7GdWZPZel6bD" data-outputId="51ab4dca-26aa-4f6b-9a35-023ff60cf09f">
<div class="sourceCode" id="cb23"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Code Cell Description: Displaying the performance metrics for the 4 models</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Function compute different metrics for different models on Test Data</span></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_metrics(model, X_test, y_test):</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model.predict(X_test)</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>    precision <span class="op">=</span> precision_score(y_test, y_pred, average<span class="op">=</span><span class="st">&#39;weighted&#39;</span>) <span class="co"># calculating precision (weighted average method - to handle class imbalance)</span></span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>    recall <span class="op">=</span> recall_score(y_test, y_pred, average<span class="op">=</span><span class="st">&#39;weighted&#39;</span>) <span class="co"># calculating Recall (weighted average method - to handle class imbalance)</span></span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>    f1 <span class="op">=</span> f1_score(y_test, y_pred, average<span class="op">=</span><span class="st">&#39;weighted&#39;</span>) <span class="co"># calculating F1 (weighted average method - to handle class imbalance)</span></span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>    accuracy <span class="op">=</span> accuracy_score(y_test, y_pred)</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>    misclassification_rate <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> accuracy</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> precision, recall, f1, accuracy, misclassification_rate</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a><span class="co"># computing and evaluating all the metrics on different algorithms on Test Data</span></span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>knn_metrics <span class="op">=</span> compute_metrics(knn, X_test, y_test)</span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>nb_metrics <span class="op">=</span> compute_metrics(nb, X_test, y_test)</span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>rf_metrics <span class="op">=</span> compute_metrics(rf, X_test, y_test)</span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a>ada_metrics <span class="op">=</span> compute_metrics(ada, X_test, y_test)</span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Dataframe of results (performance metrics) of different models on Test Data</span></span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> pd.DataFrame({</span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;Model&#39;</span>: [<span class="st">&#39;KNN&#39;</span>, <span class="st">&#39;Naive Bayes&#39;</span>, <span class="st">&#39;Random Forest&#39;</span>, <span class="st">&#39;AdaBoost&#39;</span>],</span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;Precision&#39;</span>: [knn_metrics[<span class="dv">0</span>], nb_metrics[<span class="dv">0</span>], rf_metrics[<span class="dv">0</span>], ada_metrics[<span class="dv">0</span>]],</span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;Recall&#39;</span>: [knn_metrics[<span class="dv">1</span>], nb_metrics[<span class="dv">1</span>], rf_metrics[<span class="dv">1</span>], ada_metrics[<span class="dv">1</span>]],</span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;F1 Score&#39;</span>: [knn_metrics[<span class="dv">2</span>], nb_metrics[<span class="dv">2</span>], rf_metrics[<span class="dv">2</span>], ada_metrics[<span class="dv">2</span>]],</span>
<span id="cb23-25"><a href="#cb23-25" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;Accuracy&#39;</span>: [knn_metrics[<span class="dv">3</span>], nb_metrics[<span class="dv">3</span>], rf_metrics[<span class="dv">3</span>], ada_metrics[<span class="dv">3</span>]],</span>
<span id="cb23-26"><a href="#cb23-26" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;Misclassification Rate&#39;</span>: [knn_metrics[<span class="dv">4</span>], nb_metrics[<span class="dv">4</span>], rf_metrics[<span class="dv">4</span>], ada_metrics[<span class="dv">4</span>]]</span>
<span id="cb23-27"><a href="#cb23-27" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb23-28"><a href="#cb23-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-29"><a href="#cb23-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(results)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>           Model  Precision    Recall  F1 Score  Accuracy  \
0            KNN   0.574903  0.637241  0.594377  0.637241   
1    Naive Bayes   0.494839  0.703448  0.580986  0.703448   
2  Random Forest   0.555321  0.642759  0.581895  0.642759   
3       AdaBoost   0.593798  0.702069  0.582837  0.702069   

   Misclassification Rate  
0                0.362759  
1                0.296552  
2                0.357241  
3                0.297931  
</code></pre>
</div>
</div>
<div class="cell markdown" id="freeDR18l72P">
<p>#3) Fine-Tuning Hyperparameters</p>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:283}"
id="vzuV9unal-wH" data-outputId="179748c9-5f8a-4cf8-dbb5-93b54e3af34a">
<div class="sourceCode" id="cb25"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Code Cell Description: Re-training the models after fine-tuning hyperparameters</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="co"># split the data into train and test in the ratio of 80:20</span></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a><span class="co"># defining different models or algorithms</span></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>knn <span class="op">=</span> KNeighborsClassifier() <span class="co"># KNN</span></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>nb <span class="op">=</span> GaussianNB() <span class="co"># Naive Bayes</span></span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>rf <span class="op">=</span> RandomForestClassifier(random_state<span class="op">=</span><span class="dv">42</span>) <span class="co"># random forest</span></span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>ada <span class="op">=</span> AdaBoostClassifier(random_state<span class="op">=</span><span class="dv">42</span>) <span class="co">#Adaboost</span></span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a><span class="co"># defining hyper parameters for KNN</span></span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>param_grid_knn <span class="op">=</span> {</span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;n_neighbors&#39;</span>: [<span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">7</span>, <span class="dv">9</span>],</span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;weights&#39;</span>: [<span class="st">&#39;uniform&#39;</span>, <span class="st">&#39;distance&#39;</span>],</span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;p&#39;</span>: [<span class="dv">1</span>, <span class="dv">2</span>]  <span class="co"># p=1 for Manhattan distance, p=2 for Euclidean distance</span></span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a><span class="co"># defining hyper parameters for Naive Bayes</span></span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a>param_grid_nb <span class="op">=</span> {</span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;var_smoothing&#39;</span>: [<span class="fl">1e-9</span>, <span class="fl">1e-8</span>, <span class="fl">1e-7</span>]  <span class="co"># Stabilization of variance</span></span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb25-23"><a href="#cb25-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-24"><a href="#cb25-24" aria-hidden="true" tabindex="-1"></a><span class="co"># defining hyper parameters for Random Forest</span></span>
<span id="cb25-25"><a href="#cb25-25" aria-hidden="true" tabindex="-1"></a>param_grid_rf <span class="op">=</span> {</span>
<span id="cb25-26"><a href="#cb25-26" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;n_estimators&#39;</span>: [<span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">200</span>],</span>
<span id="cb25-27"><a href="#cb25-27" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;max_depth&#39;</span>: [<span class="va">None</span>, <span class="dv">10</span>, <span class="dv">20</span>],</span>
<span id="cb25-28"><a href="#cb25-28" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;min_samples_split&#39;</span>: [<span class="dv">2</span>, <span class="dv">5</span>, <span class="dv">10</span>],</span>
<span id="cb25-29"><a href="#cb25-29" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;min_samples_leaf&#39;</span>: [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">4</span>]</span>
<span id="cb25-30"><a href="#cb25-30" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb25-31"><a href="#cb25-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-32"><a href="#cb25-32" aria-hidden="true" tabindex="-1"></a><span class="co"># defining hyper parameters for Adaboost</span></span>
<span id="cb25-33"><a href="#cb25-33" aria-hidden="true" tabindex="-1"></a>param_grid_ada <span class="op">=</span> {</span>
<span id="cb25-34"><a href="#cb25-34" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;n_estimators&#39;</span>: [<span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">200</span>],</span>
<span id="cb25-35"><a href="#cb25-35" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;learning_rate&#39;</span>: [<span class="fl">0.01</span>, <span class="fl">0.1</span>, <span class="dv">1</span>]</span>
<span id="cb25-36"><a href="#cb25-36" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb25-37"><a href="#cb25-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-38"><a href="#cb25-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Hyper parameter tuning for KNN using Grid Search Technique</span></span>
<span id="cb25-39"><a href="#cb25-39" aria-hidden="true" tabindex="-1"></a>grid_search_knn <span class="op">=</span> GridSearchCV(estimator<span class="op">=</span>knn, param_grid<span class="op">=</span>param_grid_knn, cv<span class="op">=</span><span class="dv">5</span>, n_jobs<span class="op">=-</span><span class="dv">1</span>, verbose<span class="op">=</span><span class="dv">1</span>, scoring<span class="op">=</span><span class="st">&#39;accuracy&#39;</span>)</span>
<span id="cb25-40"><a href="#cb25-40" aria-hidden="true" tabindex="-1"></a>grid_search_knn.fit(X_train, y_train)</span>
<span id="cb25-41"><a href="#cb25-41" aria-hidden="true" tabindex="-1"></a>best_knn <span class="op">=</span> grid_search_knn.best_estimator_</span>
<span id="cb25-42"><a href="#cb25-42" aria-hidden="true" tabindex="-1"></a>best_knn_params <span class="op">=</span> grid_search_knn.best_params_</span>
<span id="cb25-43"><a href="#cb25-43" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Best Parameters for KNN:&quot;</span>, best_knn_params, <span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)</span>
<span id="cb25-44"><a href="#cb25-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-45"><a href="#cb25-45" aria-hidden="true" tabindex="-1"></a><span class="co"># Hyper parameter tuning for Naive Bayes using Grid Search Technique</span></span>
<span id="cb25-46"><a href="#cb25-46" aria-hidden="true" tabindex="-1"></a>grid_search_nb <span class="op">=</span> GridSearchCV(estimator<span class="op">=</span>nb, param_grid<span class="op">=</span>param_grid_nb, cv<span class="op">=</span><span class="dv">5</span>, n_jobs<span class="op">=-</span><span class="dv">1</span>, verbose<span class="op">=</span><span class="dv">1</span>, scoring<span class="op">=</span><span class="st">&#39;accuracy&#39;</span>)</span>
<span id="cb25-47"><a href="#cb25-47" aria-hidden="true" tabindex="-1"></a>grid_search_nb.fit(X_train, y_train)</span>
<span id="cb25-48"><a href="#cb25-48" aria-hidden="true" tabindex="-1"></a>best_nb <span class="op">=</span> grid_search_nb.best_estimator_</span>
<span id="cb25-49"><a href="#cb25-49" aria-hidden="true" tabindex="-1"></a>best_nb_params <span class="op">=</span> grid_search_nb.best_params_</span>
<span id="cb25-50"><a href="#cb25-50" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Best Parameters for Naive Bayes:&quot;</span>, best_nb_params, <span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)</span>
<span id="cb25-51"><a href="#cb25-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-52"><a href="#cb25-52" aria-hidden="true" tabindex="-1"></a><span class="co"># Hyper parameter tuning for Random forest using Grid Search Technique</span></span>
<span id="cb25-53"><a href="#cb25-53" aria-hidden="true" tabindex="-1"></a>grid_search_rf <span class="op">=</span> GridSearchCV(estimator<span class="op">=</span>rf, param_grid<span class="op">=</span>param_grid_rf, cv<span class="op">=</span><span class="dv">5</span>, n_jobs<span class="op">=-</span><span class="dv">1</span>, verbose<span class="op">=</span><span class="dv">1</span>, scoring<span class="op">=</span><span class="st">&#39;accuracy&#39;</span>)</span>
<span id="cb25-54"><a href="#cb25-54" aria-hidden="true" tabindex="-1"></a>grid_search_rf.fit(X_train, y_train)</span>
<span id="cb25-55"><a href="#cb25-55" aria-hidden="true" tabindex="-1"></a>best_rf <span class="op">=</span> grid_search_rf.best_estimator_</span>
<span id="cb25-56"><a href="#cb25-56" aria-hidden="true" tabindex="-1"></a>best_rf_params <span class="op">=</span> grid_search_rf.best_params_</span>
<span id="cb25-57"><a href="#cb25-57" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Best Parameters for Random Forest:&quot;</span>, best_rf_params, <span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)</span>
<span id="cb25-58"><a href="#cb25-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-59"><a href="#cb25-59" aria-hidden="true" tabindex="-1"></a><span class="co"># Hyper parameter tuning for Adaboost using Grid Search Technique</span></span>
<span id="cb25-60"><a href="#cb25-60" aria-hidden="true" tabindex="-1"></a>grid_search_ada <span class="op">=</span> GridSearchCV(estimator<span class="op">=</span>ada, param_grid<span class="op">=</span>param_grid_ada, cv<span class="op">=</span><span class="dv">5</span>, n_jobs<span class="op">=-</span><span class="dv">1</span>, verbose<span class="op">=</span><span class="dv">1</span>, scoring<span class="op">=</span><span class="st">&#39;accuracy&#39;</span>)</span>
<span id="cb25-61"><a href="#cb25-61" aria-hidden="true" tabindex="-1"></a>grid_search_ada.fit(X_train, y_train)</span>
<span id="cb25-62"><a href="#cb25-62" aria-hidden="true" tabindex="-1"></a>best_ada <span class="op">=</span> grid_search_ada.best_estimator_</span>
<span id="cb25-63"><a href="#cb25-63" aria-hidden="true" tabindex="-1"></a>best_ada_params <span class="op">=</span> grid_search_ada.best_params_</span>
<span id="cb25-64"><a href="#cb25-64" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Best Parameters for AdaBoost:&quot;</span>, best_ada_params, <span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)</span>
<span id="cb25-65"><a href="#cb25-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-66"><a href="#cb25-66" aria-hidden="true" tabindex="-1"></a><span class="co"># defining the scoring parameters (weighted for class imbalance handling)</span></span>
<span id="cb25-67"><a href="#cb25-67" aria-hidden="true" tabindex="-1"></a>scoring <span class="op">=</span> {</span>
<span id="cb25-68"><a href="#cb25-68" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;accuracy&#39;</span>: make_scorer(accuracy_score),</span>
<span id="cb25-69"><a href="#cb25-69" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;f1&#39;</span>: make_scorer(f1_score, average<span class="op">=</span><span class="st">&#39;weighted&#39;</span>)</span>
<span id="cb25-70"><a href="#cb25-70" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb25-71"><a href="#cb25-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-72"><a href="#cb25-72" aria-hidden="true" tabindex="-1"></a><span class="co"># 5 fold cross validation of train data for KNN Classifier</span></span>
<span id="cb25-73"><a href="#cb25-73" aria-hidden="true" tabindex="-1"></a>best_knn_accuracy <span class="op">=</span> cross_val_score(best_knn, X_train, y_train, cv<span class="op">=</span><span class="dv">5</span>, scoring<span class="op">=</span><span class="st">&#39;accuracy&#39;</span>)</span>
<span id="cb25-74"><a href="#cb25-74" aria-hidden="true" tabindex="-1"></a>best_knn_f1 <span class="op">=</span> cross_val_score(best_knn, X_train, y_train, cv<span class="op">=</span><span class="dv">5</span>, scoring<span class="op">=</span><span class="st">&#39;f1_weighted&#39;</span>)</span>
<span id="cb25-75"><a href="#cb25-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-76"><a href="#cb25-76" aria-hidden="true" tabindex="-1"></a><span class="co"># 5 fold cross validation of train data for Naive bayes Classifier</span></span>
<span id="cb25-77"><a href="#cb25-77" aria-hidden="true" tabindex="-1"></a>best_nb_accuracy <span class="op">=</span> cross_val_score(best_nb, X_train, y_train, cv<span class="op">=</span><span class="dv">5</span>, scoring<span class="op">=</span><span class="st">&#39;accuracy&#39;</span>)</span>
<span id="cb25-78"><a href="#cb25-78" aria-hidden="true" tabindex="-1"></a>best_nb_f1 <span class="op">=</span> cross_val_score(best_nb, X_train, y_train, cv<span class="op">=</span><span class="dv">5</span>, scoring<span class="op">=</span><span class="st">&#39;f1_weighted&#39;</span>)</span>
<span id="cb25-79"><a href="#cb25-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-80"><a href="#cb25-80" aria-hidden="true" tabindex="-1"></a><span class="co"># 5 fold cross validation of train data for Random forest Classifier</span></span>
<span id="cb25-81"><a href="#cb25-81" aria-hidden="true" tabindex="-1"></a>best_rf_accuracy <span class="op">=</span> cross_val_score(best_rf, X_train, y_train, cv<span class="op">=</span><span class="dv">5</span>, scoring<span class="op">=</span><span class="st">&#39;accuracy&#39;</span>)</span>
<span id="cb25-82"><a href="#cb25-82" aria-hidden="true" tabindex="-1"></a>best_rf_f1 <span class="op">=</span> cross_val_score(best_rf, X_train, y_train, cv<span class="op">=</span><span class="dv">5</span>, scoring<span class="op">=</span><span class="st">&#39;f1_weighted&#39;</span>)</span>
<span id="cb25-83"><a href="#cb25-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-84"><a href="#cb25-84" aria-hidden="true" tabindex="-1"></a><span class="co"># 5 fold cross validation of train data for Adaboost Classifier</span></span>
<span id="cb25-85"><a href="#cb25-85" aria-hidden="true" tabindex="-1"></a>best_ada_accuracy <span class="op">=</span> cross_val_score(best_ada, X_train, y_train, cv<span class="op">=</span><span class="dv">5</span>, scoring<span class="op">=</span><span class="st">&#39;accuracy&#39;</span>)</span>
<span id="cb25-86"><a href="#cb25-86" aria-hidden="true" tabindex="-1"></a>best_ada_f1 <span class="op">=</span> cross_val_score(best_ada, X_train, y_train, cv<span class="op">=</span><span class="dv">5</span>, scoring<span class="op">=</span><span class="st">&#39;f1_weighted&#39;</span>)</span>
<span id="cb25-87"><a href="#cb25-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-88"><a href="#cb25-88" aria-hidden="true" tabindex="-1"></a><span class="co"># Training the models</span></span>
<span id="cb25-89"><a href="#cb25-89" aria-hidden="true" tabindex="-1"></a>best_knn.fit(X_train, y_train) <span class="co"># KNN training</span></span>
<span id="cb25-90"><a href="#cb25-90" aria-hidden="true" tabindex="-1"></a>best_nb.fit(X_train, y_train) <span class="co"># Naive Bayes Training</span></span>
<span id="cb25-91"><a href="#cb25-91" aria-hidden="true" tabindex="-1"></a>best_rf.fit(X_train, y_train) <span class="co"># Random Forest Training</span></span>
<span id="cb25-92"><a href="#cb25-92" aria-hidden="true" tabindex="-1"></a>best_ada.fit(X_train, y_train) <span class="co"># adaboost Training</span></span></code></pre></div>
<div class="output stream stdout">
<pre><code>Fitting 5 folds for each of 16 candidates, totalling 80 fits
Best Parameters for KNN: {&#39;n_neighbors&#39;: 9, &#39;p&#39;: 1, &#39;weights&#39;: &#39;uniform&#39;} 

Fitting 5 folds for each of 3 candidates, totalling 15 fits
Best Parameters for Naive Bayes: {&#39;var_smoothing&#39;: 1e-09} 

Fitting 5 folds for each of 81 candidates, totalling 405 fits
Best Parameters for Random Forest: {&#39;max_depth&#39;: 10, &#39;min_samples_leaf&#39;: 4, &#39;min_samples_split&#39;: 10, &#39;n_estimators&#39;: 50} 

Fitting 5 folds for each of 9 candidates, totalling 45 fits
Best Parameters for AdaBoost: {&#39;learning_rate&#39;: 0.01, &#39;n_estimators&#39;: 50} 

</code></pre>
</div>
<div class="output execute_result" data-execution_count="57">
<style>#sk-container-id-9 {color: black;}#sk-container-id-9 pre{padding: 0;}#sk-container-id-9 div.sk-toggleable {background-color: white;}#sk-container-id-9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-9 label.sk-toggleable__label-arrow:before {content: "â–¸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "â–¾";}#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-9 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-9 div.sk-item {position: relative;z-index: 1;}#sk-container-id-9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-9 div.sk-item::before, #sk-container-id-9 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-9 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-9 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-9 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-9 div.sk-label-container {text-align: center;}#sk-container-id-9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-9 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-9" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>AdaBoostClassifier(learning_rate=0.01, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-9" type="checkbox" checked><label for="sk-estimator-id-9" class="sk-toggleable__label sk-toggleable__label-arrow">AdaBoostClassifier</label><div class="sk-toggleable__content"><pre>AdaBoostClassifier(learning_rate=0.01, random_state=42)</pre></div></div></div></div></div>
</div>
</div>
<section id="4-performance-evaluation" class="cell markdown"
id="HlUlGn8emAL2">
<h1>4) Performance Evaluation</h1>
</section>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="mpW7CEAFmJZh" data-outputId="7d7523a4-6fe1-4643-c60f-0f6b73b58318">
<div class="sourceCode" id="cb27"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Code Cell Description: Displaying the performance metrics for the 4 models</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Function compute different metrics for different models on Test Data</span></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_metrics(model, X_test, y_test):</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model.predict(X_test)</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>    precision <span class="op">=</span> precision_score(y_test, y_pred, average<span class="op">=</span><span class="st">&#39;weighted&#39;</span>)</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>    recall <span class="op">=</span> recall_score(y_test, y_pred, average<span class="op">=</span><span class="st">&#39;weighted&#39;</span>)</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>    f1 <span class="op">=</span> f1_score(y_test, y_pred, average<span class="op">=</span><span class="st">&#39;weighted&#39;</span>)</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>    accuracy <span class="op">=</span> accuracy_score(y_test, y_pred)</span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>    misclassification_rate <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> accuracy</span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> precision, recall, f1, accuracy, misclassification_rate</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a><span class="co"># computing and evaluating all the metrics on different algorithms on Test Data</span></span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>knn_metrics <span class="op">=</span> compute_metrics(best_knn, X_test, y_test) <span class="co"># testing KNN</span></span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>nb_metrics <span class="op">=</span> compute_metrics(best_nb, X_test, y_test) <span class="co"># testing Naive Bayes</span></span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a>rf_metrics <span class="op">=</span> compute_metrics(best_rf, X_test, y_test) <span class="co"># testing Random forest</span></span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a>ada_metrics <span class="op">=</span> compute_metrics(best_ada, X_test, y_test) <span class="co"># Testing Adaboost</span></span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Dataframe of results (performance metrics) of different models on Test Data</span></span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> pd.DataFrame({</span>
<span id="cb27-21"><a href="#cb27-21" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;Model&#39;</span>: [<span class="st">&#39;KNN&#39;</span>, <span class="st">&#39;Naive Bayes&#39;</span>, <span class="st">&#39;Random Forest&#39;</span>, <span class="st">&#39;AdaBoost&#39;</span>],</span>
<span id="cb27-22"><a href="#cb27-22" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;Precision&#39;</span>: [knn_metrics[<span class="dv">0</span>], nb_metrics[<span class="dv">0</span>], rf_metrics[<span class="dv">0</span>], ada_metrics[<span class="dv">0</span>]],</span>
<span id="cb27-23"><a href="#cb27-23" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;Recall&#39;</span>: [knn_metrics[<span class="dv">1</span>], nb_metrics[<span class="dv">1</span>], rf_metrics[<span class="dv">1</span>], ada_metrics[<span class="dv">1</span>]],</span>
<span id="cb27-24"><a href="#cb27-24" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;F1 Score&#39;</span>: [knn_metrics[<span class="dv">2</span>], nb_metrics[<span class="dv">2</span>], rf_metrics[<span class="dv">2</span>], ada_metrics[<span class="dv">2</span>]],</span>
<span id="cb27-25"><a href="#cb27-25" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;Accuracy&#39;</span>: [knn_metrics[<span class="dv">3</span>], nb_metrics[<span class="dv">3</span>], rf_metrics[<span class="dv">3</span>], ada_metrics[<span class="dv">3</span>]],</span>
<span id="cb27-26"><a href="#cb27-26" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;Misclassification Rate&#39;</span>: [knn_metrics[<span class="dv">4</span>], nb_metrics[<span class="dv">4</span>], rf_metrics[<span class="dv">4</span>], ada_metrics[<span class="dv">4</span>]]</span>
<span id="cb27-27"><a href="#cb27-27" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb27-28"><a href="#cb27-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-29"><a href="#cb27-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(results)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>           Model  Precision    Recall  F1 Score  Accuracy  \
0            KNN   0.588998  0.668966  0.603071  0.668966   
1    Naive Bayes   0.494839  0.703448  0.580986  0.703448   
2  Random Forest   0.553678  0.699310  0.581468  0.699310   
3       AdaBoost   0.494839  0.703448  0.580986  0.703448   

   Misclassification Rate  
0                0.331034  
1                0.296552  
2                0.300690  
3                0.296552  
</code></pre>
</div>
</div>
<section id="5-comparison-and-analysis" class="cell markdown"
id="WqTiB1yemKAz">
<h1>5) Comparison and Analysis</h1>
</section>
<div class="cell markdown" id="58smDrOorPpA">
<p>KNN performs moderately well with a recall of 0.669, meaning it
captures a fair amount of the true positives, but its precision (0.589)
and F1 score (0.603) are relatively lower, indicating some issues with
false positives. The accuracy is also reasonable at 66.9%, though it has
the highest misclassification rate of 33.1%.</p>
<p>Naive Bayes has the highest recall (0.703), which suggests that it is
very good at detecting true positives. However, it has the lowest
precision (0.495), which indicates a higher number of false positives.
Its F1 score (0.581) is relatively balanced given the trade-off between
precision and recall. The accuracy is good at 70.3%, but the
misclassification rate remains moderate at 29.7%.</p>
<p>Random Forest offers a balance between precision (0.554) and recall
(0.699), leading to a balanced F1 score of 0.581. It has slightly better
precision compared to Naive Bayes but lower recall. Its accuracy (69.9%)
is slightly lower than Naive Bayes and AdaBoost, and the
misclassification rate is just above Naive Bayes at 30.1%. However, the
model tends to perform well overall due to its ability to capture more
complex patterns in the data.</p>
<p>AdaBoost performs identically to Naive Bayes in terms of recall, F1
score, accuracy, and misclassification rate. Like Naive Bayes, AdaBoost
has a high recall (0.703) but low precision (0.495). The F1 score
(0.581) is a reflection of this trade-off between precision and recall,
and the accuracy is 70.3%, with a 29.7% misclassification rate.</p>
<p>Naive Bayes or AdaBoost are recommended based on their performance.
Both models show the highest accuracy at 70.3% and the lowest
misclassification rate at 29.7%. Their recall is also the highest at
0.703, meaning they are more likely to correctly identify true positives
compared to the other models.This is particularly important in scenarios
where false negatives are costly.</p>
<p>However, the choice between Naive Bayes and AdaBoost would depend on
the nature of the problem:</p>
<p>Naive Bayes might be preferable for faster training and prediction,
especially for large datasets where simplicity is a priority.</p>
<p>AdaBoost, while more complex, might be favored when higher model
flexibility and the ability to handle a variety of weak learners are
required. Overall, AdaBoost may slightly edge out due to its ensemble
learning capability, offering more flexibility and robustness across
different datasets and possibly performing better on unseen data than
Naive Bayes.</p>
</div>
</body>
</html>
